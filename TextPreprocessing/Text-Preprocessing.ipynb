{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#About Dataset--\n",
    "#IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "#This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
    "df=pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selected portion\n",
    "df.review[3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Whole column in the dataset\n",
    "df.review.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_html_tags(txt):\n",
    "    pattern=re.compile('<.*?>') #Regular expression\n",
    "    return pattern.sub(r'',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. The filming tec...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.apply(rem_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_url(txt):\n",
    "    pattern=re.compile('https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. <br /><br />The...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_set=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    for char in punc_set:\n",
    "        txt=txt.replace(char,'')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='text? with , punctuation!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text with  punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#How much time to execute\n",
    "start=time.time()\n",
    "print(remove_punctuation(txt))\n",
    "t1=time.time()-start\n",
    "print(t1)  #It;s slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc(txt):\n",
    "    return txt.translate(str.maketrans('','',punc_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text with  punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#How much time to execute\n",
    "start=time.time()\n",
    "print(rem_punc(txt))\n",
    "t1=time.time()-start\n",
    "print(t1)  #It;s fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\nlp\\TextPreprocessing\\Text-Preprocessing.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/nlp/TextPreprocessing/Text-Preprocessing.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\textblob\\__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mblob\u001b[39;00m \u001b[39mimport\u001b[39;00m TextBlob, Word, Sentence, Blobber, WordList\n\u001b[0;32m      4\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.17.1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      5\u001b[0m __license__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMIT\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\textblob\\blob.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[1;32m---> 29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_property, requires_nltk_corpus\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextblob\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m lowerstrip, PUNCTUATION_REGEX\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\__init__.py:179\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[1;32m--> 179\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:2475\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[39m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[39m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[39m=\u001b[39m Downloader()\n\u001b[0;32m   2476\u001b[0m download \u001b[39m=\u001b[39m _downloader\u001b[39m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_download_dir()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:1071\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mfor\u001b[39;00m nltkdir \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mpath:\n\u001b[0;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(nltkdir) \u001b[39mand\u001b[39;00m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1073\u001b[0m         \u001b[39mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am 24 years old.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr_txt='I am 24 yeers olld.'\n",
    "txtblb=TextBlob(incr_txt)\n",
    "txtblb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "incr_txt=df.review[3]\n",
    "print(incr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Take) thinks there's a combine in his closet & his parents are fighting all the time.<br /><br />His movie is slower than a soap opera... and suddenly, Take decides to become Lumbo and kill the combine.<br /><br />of, first of all when you're going to make a film you must Decide if its a thrilled or a drama! Is a drama the movie is watchable. Parents are diverting & arguing like in real life. And then we have Take with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thrilled spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogue. Is for the shots with Take: just ignore them.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtblb=TextBlob(incr_txt)\n",
    "txtblb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['অতএব',\n",
       " 'অথচ',\n",
       " 'অথবা',\n",
       " 'অনুযায়ী',\n",
       " 'অনেক',\n",
       " 'অনেকে',\n",
       " 'অনেকেই',\n",
       " 'অন্তত',\n",
       " 'অন্য',\n",
       " 'অবধি',\n",
       " 'অবশ্য',\n",
       " 'অর্থাত',\n",
       " 'আই',\n",
       " 'আগামী',\n",
       " 'আগে',\n",
       " 'আগেই',\n",
       " 'আছে',\n",
       " 'আজ',\n",
       " 'আদ্যভাগে',\n",
       " 'আপনার',\n",
       " 'আপনি',\n",
       " 'আবার',\n",
       " 'আমরা',\n",
       " 'আমাকে',\n",
       " 'আমাদের',\n",
       " 'আমার',\n",
       " 'আমি',\n",
       " 'আর',\n",
       " 'আরও',\n",
       " 'ই',\n",
       " 'ইত্যাদি',\n",
       " 'ইহা',\n",
       " 'উচিত',\n",
       " 'উত্তর',\n",
       " 'উনি',\n",
       " 'উপর',\n",
       " 'উপরে',\n",
       " 'এ',\n",
       " 'এঁদের',\n",
       " 'এঁরা',\n",
       " 'এই',\n",
       " 'একই',\n",
       " 'একটি',\n",
       " 'একবার',\n",
       " 'একে',\n",
       " 'এক্',\n",
       " 'এখন',\n",
       " 'এখনও',\n",
       " 'এখানে',\n",
       " 'এখানেই',\n",
       " 'এটা',\n",
       " 'এটাই',\n",
       " 'এটি',\n",
       " 'এত',\n",
       " 'এতটাই',\n",
       " 'এতে',\n",
       " 'এদের',\n",
       " 'এব',\n",
       " 'এবং',\n",
       " 'এবার',\n",
       " 'এমন',\n",
       " 'এমনকী',\n",
       " 'এমনি',\n",
       " 'এর',\n",
       " 'এরা',\n",
       " 'এল',\n",
       " 'এস',\n",
       " 'এসে',\n",
       " 'ঐ',\n",
       " 'ও',\n",
       " 'ওঁদের',\n",
       " 'ওঁর',\n",
       " 'ওঁরা',\n",
       " 'ওই',\n",
       " 'ওকে',\n",
       " 'ওখানে',\n",
       " 'ওদের',\n",
       " 'ওর',\n",
       " 'ওরা',\n",
       " 'কখনও',\n",
       " 'কত',\n",
       " 'কবে',\n",
       " 'কমনে',\n",
       " 'কয়েক',\n",
       " 'কয়েকটি',\n",
       " 'করছে',\n",
       " 'করছেন',\n",
       " 'করতে',\n",
       " 'করবে',\n",
       " 'করবেন',\n",
       " 'করলে',\n",
       " 'করলেন',\n",
       " 'করা',\n",
       " 'করাই',\n",
       " 'করায়',\n",
       " 'করার',\n",
       " 'করি',\n",
       " 'করিতে',\n",
       " 'করিয়া',\n",
       " 'করিয়ে',\n",
       " 'করে',\n",
       " 'করেই',\n",
       " 'করেছিলেন',\n",
       " 'করেছে',\n",
       " 'করেছেন',\n",
       " 'করেন',\n",
       " 'কাউকে',\n",
       " 'কাছ',\n",
       " 'কাছে',\n",
       " 'কাজ',\n",
       " 'কাজে',\n",
       " 'কারও',\n",
       " 'কারণ',\n",
       " 'কি',\n",
       " 'কিংবা',\n",
       " 'কিছু',\n",
       " 'কিছুই',\n",
       " 'কিন্তু',\n",
       " 'কী',\n",
       " 'কে',\n",
       " 'কেউ',\n",
       " 'কেউই',\n",
       " 'কেখা',\n",
       " 'কেন',\n",
       " 'কোটি',\n",
       " 'কোন',\n",
       " 'কোনও',\n",
       " 'কোনো',\n",
       " 'ক্ষেত্রে',\n",
       " 'কয়েক',\n",
       " 'খুব',\n",
       " 'গিয়ে',\n",
       " 'গিয়েছে',\n",
       " 'গিয়ে',\n",
       " 'গুলি',\n",
       " 'গেছে',\n",
       " 'গেল',\n",
       " 'গেলে',\n",
       " 'গোটা',\n",
       " 'চলে',\n",
       " 'চান',\n",
       " 'চায়',\n",
       " 'চার',\n",
       " 'চালু',\n",
       " 'চেয়ে',\n",
       " 'চেষ্টা',\n",
       " 'ছাড়া',\n",
       " 'ছাড়াও',\n",
       " 'ছিল',\n",
       " 'ছিলেন',\n",
       " 'জন',\n",
       " 'জনকে',\n",
       " 'জনের',\n",
       " 'জন্য',\n",
       " 'জন্যওজে',\n",
       " 'জানতে',\n",
       " 'জানা',\n",
       " 'জানানো',\n",
       " 'জানায়',\n",
       " 'জানিয়ে',\n",
       " 'জানিয়েছে',\n",
       " 'জে',\n",
       " 'জ্নজন',\n",
       " 'টি',\n",
       " 'ঠিক',\n",
       " 'তখন',\n",
       " 'তত',\n",
       " 'তথা',\n",
       " 'তবু',\n",
       " 'তবে',\n",
       " 'তা',\n",
       " 'তাঁকে',\n",
       " 'তাঁদের',\n",
       " 'তাঁর',\n",
       " 'তাঁরা',\n",
       " 'তাঁাহারা',\n",
       " 'তাই',\n",
       " 'তাও',\n",
       " 'তাকে',\n",
       " 'তাতে',\n",
       " 'তাদের',\n",
       " 'তার',\n",
       " 'তারপর',\n",
       " 'তারা',\n",
       " 'তারৈ',\n",
       " 'তাহলে',\n",
       " 'তাহা',\n",
       " 'তাহাতে',\n",
       " 'তাহার',\n",
       " 'তিনঐ',\n",
       " 'তিনি',\n",
       " 'তিনিও',\n",
       " 'তুমি',\n",
       " 'তুলে',\n",
       " 'তেমন',\n",
       " 'তো',\n",
       " 'তোমার',\n",
       " 'থাকবে',\n",
       " 'থাকবেন',\n",
       " 'থাকা',\n",
       " 'থাকায়',\n",
       " 'থাকে',\n",
       " 'থাকেন',\n",
       " 'থেকে',\n",
       " 'থেকেই',\n",
       " 'থেকেও',\n",
       " 'দিকে',\n",
       " 'দিতে',\n",
       " 'দিন',\n",
       " 'দিয়ে',\n",
       " 'দিয়েছে',\n",
       " 'দিয়েছেন',\n",
       " 'দিলেন',\n",
       " 'দু',\n",
       " 'দুই',\n",
       " 'দুটি',\n",
       " 'দুটো',\n",
       " 'দেওয়া',\n",
       " 'দেওয়ার',\n",
       " 'দেওয়া',\n",
       " 'দেখতে',\n",
       " 'দেখা',\n",
       " 'দেখে',\n",
       " 'দেন',\n",
       " 'দেয়',\n",
       " 'দ্বারা',\n",
       " 'ধরা',\n",
       " 'ধরে',\n",
       " 'ধামার',\n",
       " 'নতুন',\n",
       " 'নয়',\n",
       " 'না',\n",
       " 'নাই',\n",
       " 'নাকি',\n",
       " 'নাগাদ',\n",
       " 'নানা',\n",
       " 'নিজে',\n",
       " 'নিজেই',\n",
       " 'নিজেদের',\n",
       " 'নিজের',\n",
       " 'নিতে',\n",
       " 'নিয়ে',\n",
       " 'নিয়ে',\n",
       " 'নেই',\n",
       " 'নেওয়া',\n",
       " 'নেওয়ার',\n",
       " 'নেওয়া',\n",
       " 'নয়',\n",
       " 'পক্ষে',\n",
       " 'পর',\n",
       " 'পরে',\n",
       " 'পরেই',\n",
       " 'পরেও',\n",
       " 'পর্যন্ত',\n",
       " 'পাওয়া',\n",
       " 'পাচ',\n",
       " 'পারি',\n",
       " 'পারে',\n",
       " 'পারেন',\n",
       " 'পি',\n",
       " 'পেয়ে',\n",
       " 'পেয়্র্',\n",
       " 'প্রতি',\n",
       " 'প্রথম',\n",
       " 'প্রভৃতি',\n",
       " 'প্রযন্ত',\n",
       " 'প্রাথমিক',\n",
       " 'প্রায়',\n",
       " 'প্রায়',\n",
       " 'ফলে',\n",
       " 'ফিরে',\n",
       " 'ফের',\n",
       " 'বক্তব্য',\n",
       " 'বদলে',\n",
       " 'বন',\n",
       " 'বরং',\n",
       " 'বলতে',\n",
       " 'বলল',\n",
       " 'বললেন',\n",
       " 'বলা',\n",
       " 'বলে',\n",
       " 'বলেছেন',\n",
       " 'বলেন',\n",
       " 'বসে',\n",
       " 'বহু',\n",
       " 'বা',\n",
       " 'বাদে',\n",
       " 'বার',\n",
       " 'বি',\n",
       " 'বিনা',\n",
       " 'বিভিন্ন',\n",
       " 'বিশেষ',\n",
       " 'বিষয়টি',\n",
       " 'বেশ',\n",
       " 'বেশি',\n",
       " 'ব্যবহার',\n",
       " 'ব্যাপারে',\n",
       " 'ভাবে',\n",
       " 'ভাবেই',\n",
       " 'মতো',\n",
       " 'মতোই',\n",
       " 'মধ্যভাগে',\n",
       " 'মধ্যে',\n",
       " 'মধ্যেই',\n",
       " 'মধ্যেও',\n",
       " 'মনে',\n",
       " 'মাত্র',\n",
       " 'মাধ্যমে',\n",
       " 'মোট',\n",
       " 'মোটেই',\n",
       " 'যখন',\n",
       " 'যত',\n",
       " 'যতটা',\n",
       " 'যথেষ্ট',\n",
       " 'যদি',\n",
       " 'যদিও',\n",
       " 'যা',\n",
       " 'যাঁর',\n",
       " 'যাঁরা',\n",
       " 'যাওয়া',\n",
       " 'যাওয়ার',\n",
       " 'যাওয়া',\n",
       " 'যাকে',\n",
       " 'যাচ্ছে',\n",
       " 'যাতে',\n",
       " 'যাদের',\n",
       " 'যান',\n",
       " 'যাবে',\n",
       " 'যায়',\n",
       " 'যার',\n",
       " 'যারা',\n",
       " 'যিনি',\n",
       " 'যে',\n",
       " 'যেখানে',\n",
       " 'যেতে',\n",
       " 'যেন',\n",
       " 'যেমন',\n",
       " 'র',\n",
       " 'রকম',\n",
       " 'রয়েছে',\n",
       " 'রাখা',\n",
       " 'রেখে',\n",
       " 'লক্ষ',\n",
       " 'শুধু',\n",
       " 'শুরু',\n",
       " 'সঙ্গে',\n",
       " 'সঙ্গেও',\n",
       " 'সব',\n",
       " 'সবার',\n",
       " 'সমস্ত',\n",
       " 'সম্প্রতি',\n",
       " 'সহ',\n",
       " 'সহিত',\n",
       " 'সাধারণ',\n",
       " 'সামনে',\n",
       " 'সি',\n",
       " 'সুতরাং',\n",
       " 'সে',\n",
       " 'সেই',\n",
       " 'সেখান',\n",
       " 'সেখানে',\n",
       " 'সেটা',\n",
       " 'সেটাই',\n",
       " 'সেটাও',\n",
       " 'সেটি',\n",
       " 'স্পষ্ট',\n",
       " 'স্বয়ং',\n",
       " 'হইতে',\n",
       " 'হইবে',\n",
       " 'হইয়া',\n",
       " 'হওয়া',\n",
       " 'হওয়ায়',\n",
       " 'হওয়ার',\n",
       " 'হচ্ছে',\n",
       " 'হত',\n",
       " 'হতে',\n",
       " 'হতেই',\n",
       " 'হন',\n",
       " 'হবে',\n",
       " 'হবেন',\n",
       " 'হয়',\n",
       " 'হয়তো',\n",
       " 'হয়নি',\n",
       " 'হয়ে',\n",
       " 'হয়েই',\n",
       " 'হয়েছিল',\n",
       " 'হয়েছে',\n",
       " 'হয়েছেন',\n",
       " 'হল',\n",
       " 'হলে',\n",
       " 'হলেই',\n",
       " 'হলেও',\n",
       " 'হলো',\n",
       " 'হাজার',\n",
       " 'হিসাবে',\n",
       " 'হৈলে',\n",
       " 'হোক',\n",
       " 'হয়']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\nlp\\TextPreprocessing\\Text-Preprocessing.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/nlp/TextPreprocessing/Text-Preprocessing.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/nlp/TextPreprocessing/Text-Preprocessing.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\__init__.py:179\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[1;32m--> 179\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:2475\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[39m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[39m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[39m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[39m=\u001b[39m Downloader()\n\u001b[0;32m   2476\u001b[0m download \u001b[39m=\u001b[39m _downloader\u001b[39m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_download_dir()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\downloader.py:1072\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[39m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m# variety of system-wide locations.\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39mfor\u001b[39;00m nltkdir \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mpath:\n\u001b[1;32m-> 1072\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(nltkdir) \u001b[39mand\u001b[39;00m nltk\u001b[39m.\u001b[39;49minternals\u001b[39m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1073\u001b[0m         \u001b[39mreturn\u001b[39;00m nltkdir\n\u001b[0;32m   1075\u001b[0m \u001b[39m# On Windows, use %APPDATA%\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'internals' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stop_words(txt):\n",
    "    new_txt=[]\n",
    "    for words in txt:\n",
    "        if words in stopwords.words('english'):\n",
    "            new_txt.append('')\n",
    "        else:\n",
    "            new_txt.append(words)\n",
    "    x=new_txt[:]\n",
    "    new_txt.clear()\n",
    "    return \" \".join(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rem_stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\nlp\\TextPreprocessing\\Text-Preprocessing.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/nlp/TextPreprocessing/Text-Preprocessing.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mreview\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: rem_stop_words(x))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1144\u001b[0m             values,\n\u001b[0;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mf:\\nlp\\TextPreprocessing\\Text-Preprocessing.ipynb Cell 34\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/nlp/TextPreprocessing/Text-Preprocessing.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mreview\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: rem_stop_words(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rem_stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "df.review.apply(lambda x: rem_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##use split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'bad', 'boy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenize\n",
    "txt1='I am a bad boy'\n",
    "txt1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am showman', 'I am from Chittagong', 'I am 24', '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenize\n",
    "txt2='I am showman.I am from Chittagong.I am 24.'\n",
    "txt2.split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where are you going?I am singing', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now Problems with split function\n",
    "txt3='Where are you going?I am singing.'\n",
    "txt3.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'from', 'delhi', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt4='I am from delhi!'\n",
    "word_tokenize(txt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'M.sc', '.', 'in', 'A.I']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt5='I have a M.sc. in A.I'\n",
    "word_tokenize(txt5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are you doing man?', 'I am dancing now.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt6='What are you doing man? I am dancing now.'\n",
    "sent_tokenize(txt6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\showm\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "M.sc\n",
      ".\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(txt5)\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "showman\n",
      ".\n",
      "I\n",
      "am\n",
      "from\n",
      "Chittagong\n",
      ".\n",
      "I\n",
      "am\n",
      "24\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(txt2)\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "email\n",
      "i\n",
      "d\n",
      "is\n",
      "das@gmail.com\n"
     ]
    }
   ],
   "source": [
    "sent='My email id is das@gmail.com'\n",
    "doc2=nlp(sent)\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_words(txt):\n",
    "    return \" \".join([ps.stem(word) for word in txt.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words='walk walks walking'\n",
    "stem_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send text onlin without worri about phone bills. free sm to hundr of gsm oper worldwid'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt='Send text online without worrying about phone bills. Free SMS to hundreds of GSM operators worldwide'\n",
    "stem_words(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\n",
    "The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the whole paragraph in sentences\n",
    "sent=sent_tokenize(paragraph)\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n",
      "[\"There 's thin line likably old-fashioned fuddy-duddy , The Count Monte Cristo ... never quite settle either side .\", 'The Rotten Tomatoes movie review dataset corpus movie review used sentiment analysis , originally collected Pang Lee [ 1 ] .', 'In work sentiment treebanks , Socher et al .', \"[ 2 ] used Amazon 's Mechanical Turk create fine-grained label parsed phrase corpus .\", 'This competition present chance benchmark sentiment-analysis idea Rotten Tomatoes dataset .', 'You asked label phrase scale five value : negative , somewhat negative , neutral , somewhat positive , positive .', 'Obstacles like sentence negation , sarcasm , terseness , language ambiguity , many others make task challenging .']\n"
     ]
    }
   ],
   "source": [
    "#lemmatizing\n",
    "for i in range(len(sent)):\n",
    "    words=word_tokenize(sent[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words  if word not in set(stopwords.words('english'))]\n",
    "    sent[i]= ' '.join(words)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
